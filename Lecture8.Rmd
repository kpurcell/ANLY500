---
title: 'Lecture 8: Regression, Correlation, Reproducible Research and Dynamic Documents'
author: "KM Purcell"
date: "October 28, 2015"
output: 
  revealjs::revealjs_presentation:
     theme: serif
     center: true
     self_contained: true
     highlight: default
---

## Lecture 7 Wrap-Up

- Scientific theories
- One-way and Two-way tests
- Non-parametric two-way tests
- Regression background
- Introduction to `lm()`


# Regression II

## Questions answered by regression

1. Which predictors ($X_1, X_2, \dots, X_p$) are important in predicting the response variable ($Y$)?
2. How many *predictor* variables explain variability in the *response* ($Y$)?
3. How well does the model fit the data?
4. Given a set of *predictor* values, what *response* value should we predict, and how accurate is that prediction?


## Question 1: which predictors are important
- In **simple regression** to evaluate if a relationship exists b/w *response* and *predictor* we evaluate if $\beta_1 = 0$.
- In a **multiple regression** framework with $p$ predictors you ask if all the *regression coefficients* are equal to zero.
$$ \beta_1 = \beta_2 = \dots = \beta_p $$

## Hypothesis testing
- In **simple** regression:
$$ H_o: \beta_1 = 0 $$
versus:
$$ H_a:\beta_1 \neq 0 $$

- In **multiple** regression:
$$ H_o:\beta_1 = \beta_2 = \dots = \beta_p = 0 $$
versus:
$$ H_a:\exists \beta_j \neq 0 $$

## Multiple regression hypothesis testing
- This hypothesis test is performed by computing the *F-statistic*
- calculated as:
$$ F = \frac{(TSS - RSS)/p}{RSS/(n - p -1)} $$

- where $TSS = \sum(y_i - \bar{y})^2$, and $RSS = \sum(y_i - \hat{y_i})^2$

## Model assumptions
- If your model does not violate assumptions, 
- no relationship between *respone* and *predictors* would produce and *F-statistics* close to 1.  
- if $H_a$ is true we expect $F > 1$
- **Lets test that with an example**


## Advertising data example
```{r, echo=TRUE, fig.width=5, fig.height=5, warning=FALSE, message=FALSE}
data <- read.csv("Advertising.csv")
data <- data[,2:5]
names(data)
head(data)
```


## EDA
```{r, echo=TRUE, fig.width=5, fig.height=5, warning=FALSE, message=FALSE}
pairs(data)
```


## Preliminary Model
```{r, echo=TRUE, warning=FALSE, message=FALSE}
mod1 <- lm(data$Sales ~ data$TV)
summary(mod1)
```


## How large is $F$ to reject $H_o$
- depends on the number of replicates ($n$) and model parameters ($p$)
- If $n$ is large and $F$ is only slightly greater than 1 you may still be able to reject $H_o$
- Alternatively, a larger $F$ value is required to reject $H_o$ when $n$ is small.
- **Note:** the performance of a regression model is highly dependent on the ratio of $n$ to $p$.


## Preliminary Model 2
```{r, echo=TRUE, warning=FALSE, message=FALSE}
mod2 <- lm(data$Sales ~ data$TV + data$Radio + data$Newspaper)
summary(mod2)
```


## Dynamic reporting using `stargazer`
```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
library(stargazer)
stargazer(mod1, mod2,  type = "html")
```


## Preliminary Model 2
```{r, echo=TRUE, warning=FALSE, message=FALSE}
mod3 <- lm(data$Sales ~ data$TV + data$Radio)
summary(mod3)
```


## Dynamic reporting 2 using `stargazer`
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
library(stargazer)
stargazer(mod2, mod3,  type = "text")
```


## Question 2: Choosing important variables
- The formulation of a single model containing only *predictors* that are associated with the *response* is termed **variable selection**
- First step in multiple regression is calculating $F$-statistics and their associated $p$-values.
- Armed with that information how do you decide which variables to include?
- Could use p-values but if the $p$ is large could lead to false discoveries.


## Stepwise variable selection
- Ideal solution is the iteratively build the model evaluating after each iteration.
- So a model with $p=2$ would evaluate four models: (1) no predictors, (2) model with $X_1$, (2) a model with $X_2$, and finally (4) a model with $X_1$ and $X_2$.
- We would choose the best model from the iteration, but **how* do we define best?


## Model quality statistics
- *Mallow's $C_p$*
- *Akaike information criterion (AIC)*
- *Bayesian information criterion (BIC)*
- *adjusted $R^2$*


## Model quality statistics
- there is a lot of material on model quality stats.
- we will start with using *adjusted $R^2$* and *AIC*
- Have you guys hear of either of these?

## Stepwise
```{r, echo=TRUE, warning=FALSE}
library(MASS)
full.lm <- lm(Sales ~ ., data = data)
step.lm <- stepAIC(full.lm, trace = TRUE)
```

## ANOVA
```{r}
step.lm$anova
```


## Parameter explosion 
- if there are $2^p$ models that contain a subset of $p$ variables that are important.  
- Trying to identify those $p$ can become *infeasaible*
- For instance, when $p=2$ then $2^2=4$.
- Well what if $p=30$.

> - Then $2^{30} = 1,073,741,824$ models


## Parameter explosion
- obviously it is intractable to calculate $1.07 \times 10^{9}$ models
- Whe require an automated solution to evaluate a set of those models: *forward selection*, *backward selection*, and *mixed selection*

## Forward selection
- begins with the null model - a model with an intercept and no predictors
- then fit $p$ simple linear regression models and add to the null model the variable that had the *lowest* RSS.
- Than add to that model the variable with the lowest RSS for the new two-variable model
- This continues until a *stopping rule* or some threshold is met.

## Backward selection
- begins with the *full model* and removes variables with the largest p-value (i.e. the varaible that is the least statistically significant).
- the new $(p - 1)$ variable model is fit and the variable with the lartest p-value is removed.
- This continues until a *stopping rule* (ex. a model in which all remaining parameters p-value < some value)

## Mixed selection
- a combination of *forward* and *backward* selection.
- Starts with the *null model* and adds parameters similar to *forward*
- If a p-value for any certain variable rises above a threshold, than it is removed from the model
- The model continues both *forward* and *backward* until all parameters are have low p-values and all excluded variables have high p-values if added to the model.

## Question 3: Model Fit
- Two most common numerical measures of model fit are: RSE and $R^2$.
- Recall, that in *simple linear regression* the $R^2$ statistic indicates the proportion of variance in the data explained by the model.  Calculated by:
$$ R^2 = \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS} $$
- In *multiple regression* $R^2$ is equal to the squre of teh correlation b/w the response and the fitted linear model or $Cor(Y,\hat{Y})^2$.


## Question 4: Prediction
- Once the model is fit, we apply the model for prediction using:
$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots + \hat{\beta_p}x_p $$
- ...
- ...

# Reproducible Research

## Why Reproducible Research
- Independent replication is the cornerstone of scientific inquiry.
- The "ultimate standard" in scientific insight is replication.
- Maximize efficiency by minimizing redundancy

## Why Reproducible Research
- Improve work habits - planning for reproduciblity means planning "with the end in mind"
- Forces a higher-level of error proofing
- Facilitates team work and communal operations
- Lowers the effort in reproducing all or parts of an analysis or project.
- If you work centers on reproducibility it will as a result facilitate higher impacts.

## Tools for a Reproducible Workflow
- **scientific programming** computational research uses programming languages which facilitate repeatable tools for easier replication and reproducibility.
- **literate programming** using tools like *knitr* allows you to combine your analysis code and your interpretations in a single document for presentation.  Tools like *knitr* for for a number of languages and platforms.
- **Markup languages** - simplified coding syntax which allows for complex formatting and document presentation with minimal effort.


## Tools for a Reproducible Workflow
- **Version control systems** - tools such as *git* and *GitHub* allow for version of code.  When you analysis and code are fully integrated you can manage and entire project from start to final presentation.
- **Shell programs** tools like *GNU Make* and *pandoc* provide a powerful collection of functionality that can simplify the creation, management, and execution of computation analysis.

## Reproducible Workflow
![workflow](http://4.bp.blogspot.com/-u3BaetbDxtM/UeR4f8924nI/AAAAAAAAGlU/fnl_KfDSSb8/s1600/Workflow.png)


## Tips for reproducible research
- Document everythin
- Keep everything as a text file
- All files should be human readable
- Explicitly tie files together
- Have a plan to organize, store, and make your files available

## 
![fileRelations](https://dl.dropboxusercontent.com/u/62107122/fileStructure.png)


## Further inquiry
![RRRR](http://ecx.images-amazon.com/images/I/41lUHCvVLqL._SX327_BO1,204,203,200_.jpg)


# Dynamic Documents

## Hypothetical Analyst process
- collect/gather data
- import data into statistical package
- run a set of procedures to produce analysis results
- copy and paste results into a typesetting program
- add descriptions and commentary
- add figures and tables
- complete report

## Dangers/Disadvantages of this process
- Excessive human effort for transferring information between mediums
- Human effort is error prone
- Workflow difficult to record especially with GUI tools, therefore difficult to reproduce.
- Small changes in data, result in having to repeat the entire analysis procedure
- Seperates the analysis process from the writing/interpretation process, thereby creating problems of syncronization and loss of synthetic thinking.

## Solutions
- Reports can be generated dynamically, directly from program code.  


## An example
![codeex](https://dl.dropboxusercontent.com/u/62107122/codeEx.PNG)

## To product this
```{r}
sales <- c(7384, 2847, 2648, 7594, 3538, 7364)
```
- The mean value for sales was `r mean(sales)`
- The lowest sales of the year was `r min(sales)`
- The highest sales of the year was `r max(sales)`


## To product this
```{r}
sales <- c(7384, 2847, 2648, 7594, 3538, 7364,
           6585, 4839, 1234, 0923, 0934, 812)
```
- The mean value for sales was `r mean(sales)`
- The lowest sales of the year was `r min(sales)`
- The highest sales of the year was `r max(sales)`

## Same thing with graphics
```{r, fig.width=4, fig.height=4}
month <- 1:12; dat <- data.frame(month, sales)
plot(sales~month, type="b")
```


## subset the data
```{r, fig.width=4, fig.height=4}
plot(sales[month<=6]~month[month<=6], type="b")
```

## Or change the data all together
```{r, fig.width=4, fig.height=4}
month <- 12:1; dat2 <- data.frame(month, sales)
plot(dat2$month,dat2$sales,  type="b")
```

## Or compound results
```{r, fig.width=4, fig.height=4}
plot(dat2$month,dat2$sales,  type="b", col="blue")
lines(dat$month,dat$sales,  type="b", col="green")
```

## Dynamic Documents
- Simplify both the process of analysis and the generation of reports and research summaries
- Dynamic documents can be in a number of formats: pdf, presentations, word documents, laTex documents, etc
- They greatly reduce an analysts workload by automating the a **labor intensive** aspect of their job.
- The initial effort to learn these practices is returned quickly, due to the higly repetitive nature of computationally based statistical analysis.

## Further inquiry
![DD](http://ecx.images-amazon.com/images/I/41kI1dxXGfL._SX321_BO1,204,203,200_.jpg)


## Reading
- I added a reading on reproducible reseach and version control.  It is written from a scientific research practice but that is why we are here because the techniques of science are being applied in private industry creating a strong appetite for analytics specialists.

