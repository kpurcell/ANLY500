---
title: 'Theories, One & Two-Sample Tests, and Regression'
date: "October 21, 2015"
output: 
  revealjs::revealjs_presentation:
     theme: serif
     center: true
     self_contained: true
     highlight: default
---

## Lecture 6 Wrap-Up

- Completed EDA discussion
- Proposals review
- R assignment #2
- Both submitted online through Moodle


# Chapter 4: Theories in Scientific Research

## Theories

>"A scientific theory is a system of *constructs* (concepts) and *propositions* (relationships between those constructs) that collectively presents a logical, systematic, and coherent explanation of a phenomenon of interest within some assumptions and boundary conditions"

## Correlation vs Causation
- Should explain **Why** things occur, not simply predict an occurence.
- Predictions based on *predictors* are possible without explaination.
- Ex: Market fluctuations following Fed announcements.
- This prediction is a possible due to *correlation* but does not inherently explain *causation*

## Causation
- A theory explaining *causation* require an understanding of cause-effect relationships
- Establishing causation requires:
1. correlations between two constructs (concepts or variables)
2. temporal precendence (cause must precede the effect)
3. rejection of alternative hypotheses

## Causal Explanations
- Explanations can be **idiographic** or **nomothetic**.
- **idographic** explanations detail a single situation with idosyncaratic detail.  The are not *generalizable* or can not be applied in other situations
- **nomothetic** explanations attempt to explain a broad class of conditions.  However, a generalized explanations suffers from less precision and detail.

## Theory Building blocks
1. Constructs - the "what" of theories
2. Propositions - the "how" of theories
3. Logic - the "why" of theories
4. Boundary conditions/assumptions - the "who" of theories

## Constructs
- These are high level, abstractions that are chosen to explain a phenomenon.
- Used to explain both simple and complex.
- Measurable representations of constructs are called **variables**

## Propositions
- associations postulated between construts based on deductive logic.
- are stated in [declarative](https://en.wikipedia.org/wiki/Sentence_(linguistics)#By_purpose) language, mean they indicate a cause-effect relationship.
- must be **testable** and rejective if not supported by empirical evidence.
- Empirical forumulations of propositions are called **hypotheses**
- The difference between propositions (formulated theoretically) and hyptheses (formulated empirically) is detailed in Figure 4.1 (pg 27 of methods text).


## 
![fig4-1](https://dl.dropboxusercontent.com/u/62107122/Fig4-1.png)


## Logic
- provides the justification for postulated propositions
- forms the connections between constructs
- represents the "explanation" that lies at the core of theories


## Boundary conditions/Assumptions
- contextualize the use of a given theory
- They dictate how "generalizable" a theory can be


## Attributes of goog theory
- logical consistancy - do all parts make sense together?
- explanatory power - how much does a theory explain or predict?
- falsifiability - can the theory be disproved? (Popper)
- parsimony - how much can be explained with as few variables as possible "Occam's razor"


# One & Two-sample Tests


## 
>"**Analytics** is the discovery and communication of meaningful patterns in data. Especially valuable ..., analytics relies on the simultaneous application of **statistics**, **computer programming** and **operations research** to quantify performance. Analytics often favors **data visualization** to communicate insight." -Wikipedia


## One Sample Tests
- One-sample tests are based on the *assuption* that a sample is derived from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). 
- This test evaluates our data $x_1, \dots, x_n$ which are independent data from a random varaibles of a **normal distribution** $N(\mu,\sigma^2)$ with a mean ($\mu$) and variance ($\sigma$).  
- The objective is to test the *null hypothesis* that $\mu = \mu_o$.
- This test is referred to as a **t test** and is implemented in `R` using the `t.test()` function.


## `t.test()` Example
```{r}
# Joe's sales numbers
sales <- c(10223, 10023, 9899, 11023, 10001, 10040, 7989, 10567)
mean(sales)
sd(sales)
quantile(sales)
# 8 monthes ago he was given a quota of $10500
# How is he doing
```


## `t.test()` Example
```{r}
# Test how he is doing
t.test(sales, mu=10500)
```

- no support for rejecting the *null hypothesis* $\mu = \mu_o$


## Wilcoxon signed-rank test
- the `t.test()` is not overly sensitive to violations of the assumption of **normality**.
- However, if your data drastically depart from normality or it is a significant concern, then the *Wilcoxon signed-rank test* is the best option.
- The Wilcoxon signed-rank test is implemented in `R` using `wilcox.test()`
- `wilcox.test` is a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) mean that it is not based on a parameter coming from a assumed probability distribution.

## Wilcoxon signed-rank test
- methodological details of the [wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) are similar but based on replacing data with order statistics.
- Procedure: subtract $\mu_o$ and rank the difference based on absolute numerical value (i.e. ignore the sign).  Then cum the ranks assuming that the distribution is symmetrical around $\mu_o$.  
- This test is computationally challanging with a large $n$, however with large $n$ the $N(\mu, \sigma^2)$ becomes reasonable.

## `wilcox.test` Example
```{r}
wilcox.test(sales)
```
- Why is the result differenet? 
>- No specified $\mu$ value.

## `wilcox.test` Example
```{r}
wilcox.test(sales, mu=10500)
```


## Two sample $t$ test

- This test evaluate the hypothesis that two samples are derived from the same distribution with the same mean ($\mu$).  
- Two sampled tests are relatively similar to one-sampled tests
- Data are derived from two groups, both from a $N(\mu, \sigma^2)$.
- The *null hypothesis* is that $\mu_1 = \mu_2$.


## Two sample $t$ test
- The $t$ test is statistical test for which the test statistics ($t$) follows the [Student's t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution). 
- Introduced by [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) in 1908 while working at the Guiness Brewery.  
- He published his work under the psudeonym "Student", hence the tests name
- In `R` the test is executed using the `t.test()` function.

## `t.test` Example
```{r}
pre <- rnorm(10, mean=7000, sd=300)
post <- rnorm(10, mean=5000, sd=300)
dat <- data.frame(pre, post)
t.test(dat$pre, dat$post, paired=TRUE)
```

- The null hypothesis is rejected... So what does that mean?

>- The $\mu_1 \neq \mu_2$


# Regression and Correlation


## Regression Analysis
- [Regression](http://onlinestatbook.com/2/regression/intro.html) is a set of methods that predict a response variable (AKA: *dependent, criterion, outcome*) based on one or a series of predictor variables (AKA: *independent, explanatory*).
- Regression **identifies** the predictor variables that are related to the response variables, **describes** the relational form, and provides an equation for **predicting** the response variable from subsequent predictors.
<!---
This came from R in Action p167
-->
- A general form of that relationship is:
$$ Y = f(X) + \varepsilon $$
- where $f$ is a fixed unknown function, of $X_1, \dots, X_p$, and $\varepsilon$ is a random *error term*

## Regression Analysis

Type of regression | Use
------------------ | ------------------
simple linear      | predict quant res from a quant. var
polynomial         | model $f(X)$ with a $n^{th}$ order polynomial
multiple linear    | two or more explanatory variables
logistic           | Predict categorical res from two or > expl vars
Poisson            | Predict resp based on counts of 1 or > expl vars
nonlinear          | Predict quant resp 1 or > expl vars $f(X)$ is nonlinear
time-series        | Modeling time-series w/correlated errors


## Simple Linear Regression

- The simplest model for a linear regression is:
$$ Y \approx \beta_0 + \beta_1 X $$

- Where $\approx$ means "approximately modeled as"
- Here we are regressing $Y$ on $X$
- For instance:
$$ Revenue \approx \beta_0 + \beta_1 \times Coupons $$
- Parameters $\beta_0$ and $\beta_1$ are unknown values
- The *regression coefficient* ($\beta_1$) is the slope of the line.
- The regression line intercepts the $y$-axis at $\beta_0$.


## Simple Linear Regression
- After building the model on training data we produce prameter estimates: $\hat{\beta_0}$ and $\hat{\beta_1}$ with which we can predict future revenue values:
$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x $$
- where, $\hat{y}$ is a predicted value of $Y$ given that $X$ = $x$. (So $ \hat{} $ values indicate estimated parameters or predicted values).



## Estimating model coefficients

- To derive these parameter values we use a method called *least squares*
- [Least squares](https://en.wikipedia.org/wiki/Least_squares) regression is a [**optimization**](https://en.wikipedia.org/wiki/Mathematical_optimization) method that seeks to determine values for parameters $\hat{\beta_0}$ and $\hat{\beta_1}$ that *minimizes* the distance of data points from the regression line.  


## Residuals
- Given a set of data: $(x_1,y_1),(x_2,y_2), \dots, (x_n,y_n)$
- In which *revenue* ($y$) and *coupons* ($x$) are recorded for $n=10$ different regions.
- The objective is fine the intercept ($\hat{\beta_0}$) and slope ($\hat{\beta_1}$) that creates a line as close to the $n=10$ data points as possible.
- The distance between the data points and the regression line is called the *residuals*.


## Residuals
- So if: $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1$ is the prediction for $Y$ based on the $i^{th}$ value of $X$
- Then, $\varepsilon_i = y_i - \hat{y_i}$, which is the $i^{th}$ **residual**.
- That is the difference between the $i^{th}$ observed value and the $i^{th}$ value predicted from the linear model.
- The *residual sum of squares* (RSS) is defined as:
$$ RSS = \varepsilon_1^2 + \varepsilon_2^2 + \dots + \varepsilon_n^2, $$
- or,
$$ RSS = (y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 + \dots + (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)^2 $$



## Residuals Visualized
```{r, echo=FALSE, warning=F, message=F, fig.width=6, fig.height=6}
# http://www.r-bloggers.com/how-to-plot-points-regression-line-and-residuals/
x <- c(173, 169, 176, 166, 161, 164, 160, 158, 180, 187)
y <- c(80, 68, 72, 75, 70, 65, 62, 60, 85, 92) 
# plot scatterplot and the regression line
mod1 <- lm(y ~ x)
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10), main="Residuals")
abline(mod1, lwd=2)


# calculate residuals and predicted values
res <- signif(residuals(mod1), 5)
pre <- predict(mod1) 
# plot distances between points and the regression line
segments(x, y, x, pre, col="red")

# add labels (res values) to points
library(calibrate)
textxy(x, y, res, cx=0.7)
```



## Least squares regression
- least squares chooses $\hat{\beta_0}$ and $\hat{\beta_1}$ that minimize the RSS.

![RSSoptim](http://i.stack.imgur.com/bmg5Z.png)


## Executing a linear model in `R`
```
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```
- `formula` symbolic description of the model
- `data` the data frame (optional)

##
```{r}
str(mod1)
```


##
```{r}
summary(mod1)
```











 







