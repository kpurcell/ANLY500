---
title: 'Lecture 7: One & Two-Sample Tests and Regression'
date: "October 21, 2015"
output: ioslides_presentation
---

## Lecture 6 Wrap-Up

>- Completed EDA discussion
>- Proposals review
>- R assignment #2
>- Both submitted online through Moodle


# Statistics Analysis with **R**


## 
>"**Analytics** is the discovery and communication of meaningful patterns in data. Especially valuable ..., analytics relies on the simultaneous application of **statistics**, **computer programming** and **operations research** to quantify performance. Analytics often favors **data visualization** to communicate insight." -Wikipedia


## One Sample Tests
>- One-sample tests are based on the *assuption* that a sample is derived from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). 
>- This test evaluates our data $x_1, \dots, x_n$ which are independent data from a random varaibles of a **normal distribution** $N(\mu,\sigma^2)$ with a mean ($\mu$) and variance ($\sigma$).  
>- The objective is to test the *null hypothesis* that $\mu = \mu_o$.
>- This test is referred to as a **t test** and is implemented in `R` using the `t.test()` function.


## `t.test()` Example
```{r}
# Joe's sales numbers
sales <- c(10223, 10023, 9899, 11023, 10001, 10040, 7989, 10567)
mean(sales)
sd(sales)
quantile(sales)
# 8 monthes ago he was given a quota of $10500
# How is he doing
```


## `t.test()` Example
```{r}
# Test how he is doing
t.test(sales, mu=10500)
```

- no support for rejecting the *null hypothesis* $\mu = \mu_o$


## Wilcoxon signed-rank test
>- the `t.test()` is not overly sensitive to violations of the assumption of **normality**.
>- However, if your data drastically depart from normality or it is a significant concern, then the *Wilcoxon signed-rank test* is the best option.
>- The Wilcoxon signed-rank test is implemented in `R` using `wilcox.test()`
>- `wilcox.test` is a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) mean that it is not based on a parameter coming from a assumed probability distribution.

## Wilcoxon signed-rank test
>- methodological details of the [wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) are similar but based on replacing data with order statistics.
>- Procedure: subtract $\mu_o$ and rank the difference based on absolute numerical value (i.e. ignore the sign).  Then cum the ranks assuming that the distribution is symmetrical around $\mu_o$.  
>- This test is computationally challanging with a large $n$, however with large $n$ the $N(\mu, \sigma^2)$ becomes reasonable.

## `wilcox.test` Example
```{r}
wilcox.test(sales)
```
>- Why is the result differenet? No specified $\mu$ value.

## `wilcox.test` Example
```{r}
wilcox.test(sales, mu=10500)
```


## Two sample $t$ test

>- This test evaluate the hypothesis that two samples are derived from the same distribution with the same mean ($\mu$).  
>- Two sampled tests are relatively similar to one-sampled tests
>- Data are derived from two groups, both from a $N(\mu, \sigma^2)$.
>- The *null hypothesis* is that $\mu_1 = \mu_2$.


## Two sample $t$ test
>- The $t$ test is statistical test for which the test statistics ($t$) follows the [Student's t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution). 
>- Introduced by [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) in 1908 while working at the Guiness Brewery.  
>- He published his work under the psudeonym "Student", hence the tests name
>- In `R` the test is executed using the `t.test()` function.

## `t.test` Example
```{r}
pre <- rnorm(10, mean=7000, sd=300)
post <- rnorm(10, mean=5000, sd=300)
dat <- data.frame(pre, post)
t.test(dat$pre, dat$post, paired=TRUE)
```

>- The null hypothesis is rejected... So what does that mean?

- The $\mu_1 \neq \mu_2$


## Regression Analysis

>- A core technique in statistics.
>- [Regression](http://onlinestatbook.com/2/regression/intro.html) is a set of methods that predict a response variable (AKA: *dependent, criterion, outcome*) based on one or a series of predictor variables (AKA: *independent, explanatory*).
>- Regression **identifies** the predictor variables that are related to the response variables, **describes** the relational form, and provides an equation for **predicting** the response variable from subsequent predictors.
<!---
This came from R in Action p167
-->

## Regression Analysis

>- The model for a linear regression is:
$$ y_i = \alpha + \beta x_i + \varepsilon_i $$

>- 






 







